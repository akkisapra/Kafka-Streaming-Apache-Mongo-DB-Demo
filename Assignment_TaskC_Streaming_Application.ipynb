{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream Processing using Apache Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have written a streaming application in Apache Spark Streaming which has a local streaming context with two execution threads\n",
    "and a batch interval of 10 seconds. \n",
    "Theis streaming application will receive streaming data from three producers. \n",
    "If the streaming application has data from all or at least two of the producers, it will do the processing as follows:\n",
    ">- Join the streams based on the location (i,e, latitude and longitude) and create the data model developed in Task A.\n",
    ">- Find two locations which are close to each other or not by implementing the geo-hashing algorithm using precision 5. The precision determines the number of characters in the Geohash.\n",
    ">- If we receive the data from two different satellites AQUA and TERRA for the same location, then average the ‘surface temperature’ and ‘confidence’.\n",
    ">- If the streaming application has the data from only one producer (Producer 1), it implies that there was no fire at that time and we can store the climate data into MongoDB straight away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "def deleteContents(obj):\n",
    "    obj['date']=obj['timestamp']\n",
    "    del obj['timestamp']\n",
    "    return obj\n",
    "\n",
    "#Task c2 a 3\n",
    "def only_store(data_received):\n",
    "        client = MongoClient () # method 1: connect on the default host and port\n",
    "        db=client['fit5148_assignment_db']\n",
    "        for each in data_received:\n",
    "            db['climate_historic'].insert_one(deleteContents(each))\n",
    "        client.close()\n",
    "   \n",
    "    \n",
    "def full_received(data_received): #task C2 a 2\n",
    "        client = MongoClient () # method 1: connect on the default host and port\n",
    "        db=client['fit5148_assignment_db']\n",
    "        sender2=[]\n",
    "        sender3=[]\n",
    "        sender1=[]\n",
    "        datetimeStr=str(dt.datetime.now()) \n",
    "        for elem in data_received:\n",
    "            if(elem['sender_id']==1):\n",
    "                sender1.append(elem)\n",
    "            elif(elem['sender_id']==2):\n",
    "                sender2.append(elem)\n",
    "            else:\n",
    "                sender3.append(elem)\n",
    "                \n",
    "        if(sender2[0]['geohash']==sender3[0]['geohash']):\n",
    "                temperatureAgg=(float(firstObj['surface_temperature_celcius'])+float(secondObj['surface_temperature_celcius']))/2\n",
    "                confidenceAgg=(float(firstObj['confidence'])+float(secondObj['confidence']))/2\n",
    "                sender2[0]['surface_temperature_celcius']=temperatureAgg\n",
    "                sender2[0]['confidence']=confidenceAgg\n",
    "                sender3=[]\n",
    "                \n",
    "                \n",
    "        sender1Arr=[]\n",
    "        for item in sender1:\n",
    "            insertedFlag=0\n",
    "            if(len(sender3)>1):\n",
    "                if(item[\"geohash\"]==sender3[0][\"geohash\"]):\n",
    "                    hotspot_dict={}\n",
    "                    hotspot_dict['id']=sender3[0]['_id']\n",
    "                    hotspot_dict[\"confidence\"]=sender3[0][\"confidence\"]\n",
    "                    hotspot_dict['surface_temperature_celcius']=sender3[0]['surface_temperature_celcius']\n",
    "                    sender1Arr.append(hotspot_dict)\n",
    "                    item[\"hotspot\"]=sender1Arr\n",
    "                    insertedFlag=1\n",
    "                    db['climate_historic'].insert_one(deleteContents(item))\n",
    "                    sender1Arr=[]\n",
    "                    sender3['datetime']=datetimeStr\n",
    "                    sender3['owner']=item['_id']\n",
    "                    db['hotspot_historic'].insert_one(sender3[0])\n",
    "                    \n",
    "            if(item[\"geohash\"]==sender2[0][\"geohash\"]):\n",
    "                hotspot_dict={}\n",
    "                hotspot_dict['id']=sender2[0]['_id']\n",
    "                hotspot_dict[\"confidence\"]=sender2[0][\"confidence\"]\n",
    "                hotspot_dict['surface_temperature_celcius']=sender2[0]['surface_temperature_celcius']\n",
    "                sender1Arr.append(hotspot_dict)\n",
    "                item[\"hotspot\"]=sender1Arr\n",
    "                insertedFlag=1\n",
    "                db['climate_historic'].insert_one(deleteContents(item))\n",
    "                sender1Arr=[]\n",
    "                sender2['owner']=item['_id']\n",
    "                sender2['datetime']=datetimeStr\n",
    "                db['hotspot_historic'].insert_one(sender2[0])\n",
    "                \n",
    "            if(insertedFlag==0):\n",
    "                db['climate_historic'].insert_one(deleteContents(item))\n",
    "                \n",
    "        client.close()\n",
    "    \n",
    "def two_received(data_received):  # Tack c2 a 1\n",
    "        client = MongoClient () # method 1: connect on the default host and port\n",
    "        db=client['fit5148_assignment_db']\n",
    "        sender1=[]\n",
    "        other=[]\n",
    "        \n",
    "        for elem in data_received:\n",
    "            if(elem['sender_id']==1):\n",
    "                sender1.append(elem)\n",
    "            else:\n",
    "                other.append(elem)\n",
    "                \n",
    "        sender1Arr=[]\n",
    "        for elem in sender1:\n",
    "            if(elem[\"geohash\"]==other[0][\"geohash\"]):\n",
    "                hotspot_dict={}\n",
    "                hotspot_dict['id']=other[0]['_id']\n",
    "                hotspot_dict[\"confidence\"]=other[0][\"confidence\"]\n",
    "                hotspot_dict['surface_temperature_celcius']=other[0]['surface_temperature_celcius']\n",
    "                elem[\"hotspot\"]=sender1Arr.append(hotspot_dict)\n",
    "                db['climate_historic'].insert_one(deleteContents(elem))\n",
    "                other[0]['datetime']=datetimeStr\n",
    "                other[0]['owner']=item['_id']\n",
    "                db['hotspot_historic'].insert_one(other[0])\n",
    "            else:\n",
    "                db['climate_historic'].insert_one(deleteContents(elem))\n",
    "        client.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import geohash\n",
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "#sending data to DB\n",
    "def sendDataToDB(iter):\n",
    "    data_received =[]\n",
    "    i=0\n",
    "    flag_1=0\n",
    "    flag_2=0\n",
    "    flag_3=0\n",
    "    \n",
    "    for record in iter:\n",
    "        data = json.loads(record[1])\n",
    "        data_received.append(data)\n",
    "        \n",
    "    for i in range(len(data_received)):\n",
    "        data_received[i][\"geohash\"]=geohash.encode(float(data_received[i][\"latitude\"]),\n",
    "                                                   float(data_received[i][\"longitude\"]),5)\n",
    "        if data_received[i][\"sender_id\"] ==1:\n",
    "                flag_1=1\n",
    "        if  data_received[i][\"sender_id\"] ==2:\n",
    "                flag_2=1\n",
    "        if  data_received[i][\"sender_id\"] ==3:\n",
    "                flag_3=1\n",
    "    if (flag_1 ==1 and flag_2 ==0 and flag_3==0):\n",
    "        only_store(data_received)\n",
    "    elif (flag_1==1 and flag_2==1 and flag_3==1):\n",
    "        full_received(data_received)\n",
    "    elif (flag_1 ==1 and flag_2 ==1)or (flag_1 ==1 and flag_3==1):\n",
    "        two_received(data_received)\n",
    "\n",
    "\n",
    "n_secs = 10\n",
    "topic = \"climate_data\"\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")\n",
    "sc = SparkContext.getOrCreate()\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs)\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week12-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(600) # Run stream for 10 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
